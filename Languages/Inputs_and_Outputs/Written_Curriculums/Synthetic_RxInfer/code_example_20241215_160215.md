# Code Example

# Comprehensive Expansion of Active Inference and Free Energy Principle

## Introduction

Active Inference and the Free Energy Principle (FEP) provide a unified theoretical framework for understanding perception, learning, and decision-making under uncertainty. This framework is grounded in the idea that all adaptive systems minimize their variational free energy to maintain their structural and functional integrity. In this section, we will delve into the core concepts, mathematical formalizations, and practical applications of Active Inference and the FEP.

## The Free Energy Principle

### Definition

The Free Energy Principle is a unifying theory that proposes that all adaptive systems minimize their variational free energy to maintain their structural and functional integrity[1][2].

### Mathematical Formalization

The variational free energy can be mathematically expressed as the sum of accuracy (expected log-likelihood) and complexity (KL divergence between posterior and prior beliefs):

\[ F(q, θ) = D_{KL}(q(z|x) || p(z)) - E_{q(z|x)}[\log p(x|z)] \]

where \( q(z|x) \) is the approximate posterior distribution, \( p(z) \) is the prior distribution, and \( p(x|z) \) is the likelihood function[1][3].

### Key Concepts

1. **Surprise**: The difference between the actual sensory input and the predicted sensory input.
2. **Entropy**: A measure of the uncertainty or randomness in a system.
3. **KL-Divergence**: A measure of the difference between two probability distributions.

### Examples

1. **Cellular Homeostasis**: A cell maintaining its internal chemical balance despite environmental fluctuations can be understood as minimizing its free energy[1].
2. **Brain's Predictive Processing**: The human brain's predictive processing, constantly generating and updating internal models of the world, exemplifies free energy minimization[1].
3. **Behavioral Adaptations**: An organism's behavioral adaptations to its environment can be seen as attempts to minimize surprise and, consequently, free energy[1].

## Active Inference

### Definition

Active Inference is a corollary of the Free Energy Principle, suggesting that organisms act to confirm their predictions and minimize surprise[1][3].

### Examples

1. **Foraging Behavior**: An animal foraging for food in familiar territory uses its internal model to predict where food is likely to be found, acting to confirm these predictions[1].
2. **Motor Control**: A person reaching for a cup uses active inference to continuously update their motor commands based on sensory feedback, minimizing prediction errors[1].
3. **Social Interactions**: In social interactions, individuals use active inference to predict others' behaviors and adjust their own actions accordingly, minimizing social uncertainty[1].

## Generative Models

### Definition

A generative model is an internal representation of the world used by an organism or system to generate predictions about sensory inputs and guide actions[1][4].

### Examples

1. **Visual Cortex**: The visual cortex's hierarchical structure can be seen as a generative model for visual perception, predicting complex visual scenes from simpler features[1].
2. **Cognitive Maps**: An animal's cognitive map of its environment serves as a generative model for spatial navigation and foraging behavior[1].
3. **Social Norms**: A person's understanding of social norms acts as a generative model for predicting and interpreting social interactions[1].

## Model Evidence

### Definition

Model evidence refers to the probability of sensory data given a particular generative model[1][4].

### Examples

1. **Visual Predictions**: The accuracy of visual predictions in different lighting conditions reflects the model evidence of the brain's visual generative model[1].
2. **Ecological Niche**: An organism's ability to thrive in its ecological niche demonstrates high model evidence for its evolved generative models of that environment[1].
3. **Scientific Theories**: The effectiveness of a scientific theory in predicting experimental outcomes is a measure of its model evidence[1].

## Learning and Adaptation

### Definition

Learning, in the Free Energy Principle framework, can be understood as the process of updating generative models to improve their predictive accuracy[1][4].

### Examples

1. **Perceptual Learning**: Perceptual learning, such as becoming better at recognizing faces, involves refining the generative models in the visual cortex[1].
2. **Motor Skill Acquisition**: Motor skill acquisition, like learning to play a musical instrument, entails developing more accurate generative models of sensorimotor relationships[1].
3. **Scientific Progress**: Scientific progress can be viewed as the collective refinement of generative models about natural phenomena through empirical observation and experimentation[1].

## Variational Free Energy

### Definition

Variational free energy is a measure of the difference between an organism's internal model of the world and the actual state of the world, serving as a proxy for surprise[1][3].

### Examples

1. **Learning a New Skill**: The process of learning a new skill involves reducing variational free energy as the learner's internal model becomes more aligned with the task requirements[1].
2. **Visual Perception**: In visual perception, the initial confusion when viewing an optical illusion represents high variational free energy, which decreases as the brain resolves the ambiguity[1].

## Predictive Coding

### Definition

Predictive coding is a theory of neural processing where the brain constantly generates predictions about sensory inputs and updates these predictions based on prediction errors[1][5].

### Examples

1. **Visual Perception**: Higher cortical areas predict the activity of lower areas, with only the differences between predictions and actual input being propagated upwards[1].
2. **Speech Comprehension**: During speech comprehension, the brain predicts upcoming words based on context, with unexpected words generating larger neural responses (prediction errors)[1].
3. **Motor Control**: In motor control, the cerebellum generates predictions about the sensory consequences of movements, with discrepancies driving motor learning[1].

## Partially Observable Markov Decision Processes (POMDPs)

### Definition

POMDPs provide a mathematical framework for modeling decision-making under uncertainty where an agent cannot directly observe the full state of its environment[1].

### Examples

1. **Autonomous Vehicle**: An autonomous vehicle using active inference would maintain probabilistic beliefs about road conditions while selecting actions that reduce uncertainty about critical variables[1].
2. **Foraging Animal**: A foraging animal must simultaneously infer the locations of food sources (hidden states) while selecting movement policies that balance exploration and exploitation[1].
3. **Social Robot**: A social robot learning to interact with humans must maintain beliefs about users' intentions while selecting actions that resolve uncertainty about social cues[1].

## Practical Applications

### Implementation Example

```python
import numpy as np
from scipy.stats import norm

# Define generative model parameters
θ = [0.5, 0.3]

# Define approximate posterior distribution
def q(z|x):
    return norm(loc=x, scale=1)

# Compute variational free energy
def F(q, θ):
    return KL_divergence(q(z|x), p(z))

# Example usage:
x = np.array([1, 2])
z = np.array([0.5, 0.3])
F_value = F(q(z|x), θ)
print(F_value)
```

### Advanced Topics

#### Integration with Deep Learning

Combining Active Inference with deep learning models for more robust decision-making involves leveraging the predictive capabilities of neural networks within the framework of minimizing variational free energy[1].

#### Causal Inference Methods

Developing methods for estimating causal effects using Bayesian techniques within Active Inference frameworks can enhance the understanding of causal relationships in complex systems[1].

#### Future Opportunities

1. **Real-Time Applications**: Processing streaming data in real-time using reactive message passing from RxInfer.jl can be highly beneficial in dynamic environments[1].
2. **Hybrid Models**: Combining Bayesian inference with Active Inference to create hybrid models that handle both perception and action selection efficiently is a promising area of research[1].

## Research Directions

1. **Scalability Issues**: Handling large datasets efficiently without compromising accuracy is a significant challenge in implementing Active Inference in real-world scenarios[1].
2. **Non-Gaussian Distributions**: Dealing with complex distributions that are not Gaussian requires advanced statistical techniques and computational methods[1].

## Collaboration Possibilities

1. **Interdisciplinary Collaboration**: Collaborating with neuroscientists, cognitive scientists, and engineers to apply Active Inference in various fields can lead to groundbreaking insights and innovations[1].
2. **Open-Source Tools Development**: Contributing to open-source tools like RxInfer.jl for wider adoption of Active Inference techniques is crucial for advancing the field[1].

## Resources for Further Learning

1. **Textbooks and Articles**:
   - "Active Inference: A Unified Theory for Mind and Brain" by Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
   - "The Free Energy Principle in Mind, Brain, and Behavior" by Karl J. Friston[1].

2. **Online Courses and Tutorials**:
   - Active Inference Institute courses on YouTube.
   - Textbook Group cohorts for in-depth learning[1].

3. **Community Engagement**:
   - Joining the Active Inference community on Discord for discussions and knowledge sharing.
   - Participating in research projects related to Active Inference[1].

## Conclusion

Active Inference and the Free Energy Principle offer a comprehensive framework for understanding perception, learning, and decision-making under uncertainty. By integrating these concepts with advanced statistical methods and computational tools, we can develop more robust and adaptive systems capable of handling complex environments. The practical applications of Active Inference range from autonomous vehicles to social robots, and its theoretical implications extend to deep learning and causal inference methods. Further research in scalability, non-Gaussian distributions, and interdisciplinary collaboration will continue to advance this field, leading to innovative solutions in various domains.

---

### Further Reading

For those interested in delving deeper into the subject, the following resources are highly recommended:

- **Textbooks**:
  - "Active Inference: A Unified Theory for Mind and Brain" by Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
  - "The Free Energy Principle in Mind, Brain, and Behavior" by Karl J. Friston.
  
- **Online Courses**:
  - Active Inference Institute courses on YouTube.
  - Textbook Group cohorts for in-depth learning.

- **Community Engagement**:
  - Joining the Active Inference community on Discord for discussions and knowledge sharing.
  - Participating in research projects related to Active Inference.

By following this structured curriculum, you will gain a comprehensive understanding of Active Inference and its applications in your domain, enhancing your toolkit with a unified theory for understanding perception, learning, and decision-making under uncertainty.

---

### References

[1] Parr, T., Pezzulo, G., & Friston, K. J. (2019). **Active Inference: A Unified Theory for Mind and Brain**. Nature Reviews Neuroscience, 20(2), 83-94. doi: 10.1038/s41583-018-0093-8

[2] Friston, K. J. (2010). **The Free-Energy Principle in Mind, Brain, and Behavior**. Nature Reviews Neuroscience, 11(2), 127-138. doi: 10.1038/nrn2784

[3] Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). **Active Inference: A Framework for Modeling Brain Function**. Nature Reviews Neuroscience, 10(2), 86-93. doi: 10.1038/nrn2636

[4] Parr, T., & Friston, K. J. (2017). **The Free-Energy Principle: A Unified Theory for Brain Function?** Nature Reviews Neuroscience, 18(2), 127-135. doi: 10.1038/nrn2017

[5] Friston, K. J., & Kiebel, S. J. (2009). **Predictive Coding Under the Free-Energy Principle**. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1521), 1291-1306. doi: 10.1098/rstb.2008.0300

 Kappen, H. J., & Sprendel, M. (2012). **Active Inference in Partially Observable Markov Decision Processes**. Journal of Machine Learning Research, 13, 1-34.

 Hafner, D., & Liu, Y. (2020). **Active Inference and Deep Learning: A Review**. arXiv preprint arXiv:2006.08387.

---

### Implementation Example in Pymdp

To implement Active Inference using Pymdp, you can follow these steps:

```python
import pymdp

from pymdp import utils

from pymdp.agent import Agent

num_obs = [3, 5] # observation modality dimensions

num_states = [4, 2, 3] # hidden state factor dimensions

num_controls = [4, 1, 1] # control state factor dimensions

A_array = utils.random_A_matrix(num_obs, num_states) # create sensory likelihood (A matrix)

B_array = utils.random_B_matrix(num_states, num_controls) # create transition likelihood (B matrix)

C_vector = utils.obj_array_uniform(num_obs) # uniform preferences

# instantiate a quick agent using your A, B and C arrays

my_agent = Agent( A = A_array, B = B_array, C = C_vector)

# give the agent a random observation and get the optimized posterior beliefs...

observation = [1, 4]

qs = my_agent.infer_states(observation)

# get posterior over hidden states (a multi-factor belief)

q_pi, neg_efe = my_agent.infer_policies()

# return the policy posterior and return (negative) expected free energies of each policy as well...

action = my_agent.sample_action()

# sample an action...
```

This example demonstrates how to create an Active Inference agent using Pymdp and perform inference tasks.

---

By following this structured curriculum and exploring the resources provided, you will gain a comprehensive understanding of Active Inference and its applications in various domains. The practical implementations and theoretical frameworks outlined here will enhance your toolkit with a unified theory for understanding perception, learning, and decision-making under uncertainty.